{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat,savemat\n",
    "import nibabel as nib\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import scipy.io as sio\n",
    "\n",
    "bs_size = 1000\n",
    "\n",
    "# bigger patch 5by5\n",
    "# dropout layers\n",
    "# adding channel of anatomical label for input\n",
    "# do both SH1200 and SH3000 together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['A','B','C','D','E','F','G','I','J','K']\n",
    "\n",
    "X = np.empty((5,5,5,31,0))\n",
    "Y = np.empty((1,1,1,30,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dirs)):\n",
    "#i = 0\n",
    "\n",
    "    data_path = \"/Users/kurtschilling/Data/harmonization/patches/s%s_A2D_5Patches.mat\" % (dirs[i])\n",
    "    print(data_path)\n",
    "    mat_contents = sio.loadmat(data_path)\n",
    "    #print(mat_contents)\n",
    "    \n",
    "    X1 = mat_contents['input1200_row1']; X2 = mat_contents['input1200_row2']; X3 = mat_contents['input1200_row3']\n",
    "    X4 = mat_contents['input1200_row4']; X5 = mat_contents['input1200_row5']\n",
    "\n",
    "    Z1 = mat_contents['input3000_row1']; Z2 = mat_contents['input3000_row2']; Z3 = mat_contents['input3000_row3']\n",
    "    Z4 = mat_contents['input3000_row4']; Z5 = mat_contents['input3000_row5']\n",
    "\n",
    "    Y1 = mat_contents['output1200']; Y2 = mat_contents['output3000'];\n",
    "    labels = mat_contents['labelsinput']\n",
    "    dims=X1.shape\n",
    "    print(dims)\n",
    "    print(labels.shape)\n",
    "    print(Y1.shape)\n",
    "\n",
    "    # output is output1200 output3000\n",
    "\n",
    "    XX = np.empty((5,5,5,31,dims[4]))\n",
    "    YY = np.empty((1,1,1,30,dims[4]))\n",
    "\n",
    "    XX[:,:,:,0,:] = labels[:,:,:,0,:]\n",
    "    XX[0,:,:,1:16,:] = X1; XX[1,:,:,1:16,:] = X2; XX[2,:,:,1:16,:] = X3; XX[3,:,:,1:16,:] = X4; XX[4,:,:,1:16,:] = X5\n",
    "    XX[0,:,:,16:32,:] = Z1; XX[1,:,:,16:32,:] = Z2; XX[2,:,:,16:32,:] = Z3; XX[3,:,:,16:32,:] = Z4; XX[4,:,:,16:32,:] = Z5\n",
    "\n",
    "    #print(XX[1,1,1,:,10000])\n",
    "\n",
    "    YY[:,:,:,0:15,:]=Y1; YY[:,:,:,15:31]=Y2\n",
    "\n",
    "    #print(YY[:,:,:,:,10000])\n",
    "    print(XX.shape)\n",
    "    print(YY.shape)\n",
    "    \n",
    "    # remove NAN, INF\n",
    "    from numpy import asarray as ar\n",
    "    arr1 = np.squeeze(np.isnan(YY).any(axis=3))\n",
    "    arr2 = np.squeeze(np.isinf(YY).any(axis=3))\n",
    "    arr3 = ar(arr1) | ar(arr2)\n",
    "    print(arr3.shape)\n",
    "\n",
    "    XX = XX[:,:,:,:,~arr3]\n",
    "    YY = YY[:,:,:,:,~arr3]\n",
    "    print(XX.shape)\n",
    "    print(YY.shape)\n",
    "\n",
    "    # remove >10\n",
    "    arr6 = np.squeeze(np.greater(np.abs(YY),10).any(axis=3))\n",
    "    print(arr6.shape)\n",
    "    XX = XX[:,:,:,:,~arr6]\n",
    "    YY = YY[:,:,:,:,~arr6]\n",
    "    print(XX.shape)\n",
    "    print(YY.shape)\n",
    "    \n",
    "    #########\n",
    "    remaining_samples = XX.shape\n",
    "    remaining_samples = np.rint(remaining_samples[4])\n",
    "    remaining_samples.astype(int)\n",
    "    print(remaining_samples)\n",
    "    elim_length = np.rint(remaining_samples/3)\n",
    "    elim_length.astype(int)\n",
    "    print(elim_length)\n",
    "    print(elim_length.dtype)\n",
    "    print(remaining_samples.dtype)\n",
    "    arr7 = np.random.choice(remaining_samples.astype(int), elim_length.astype(int), replace = False)\n",
    "    print(arr7.shape)\n",
    "    mask = np.ones(remaining_samples.astype(int),dtype=bool)\n",
    "    print(mask.shape)\n",
    "    mask[[arr7]]=False\n",
    "    print(mask)\n",
    "    XX = XX[:,:,:,:,mask]\n",
    "    YY = YY[:,:,:,:,mask]\n",
    "    #Xv = Xv[:,:,:,:,arr7]\n",
    "    #Xv = np.delete(X,arr7,axis=4)\n",
    "    #Yv = np.delete(Y,arr7,axis=4)\n",
    "    #Yv = Yv[:,:,:,:,arr7]\n",
    "    print(XX.shape)\n",
    "    print(YY.shape)\n",
    "    #######\n",
    "\n",
    "    X = np.append(X, XX, axis=4)\n",
    "    Y = np.append(Y, YY, axis=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del XX, YY, X1, X2, X3, X4, X5, Z1, Z2, Z3, Z4, Z5\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class SHDataSet(Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return np.amax(X.shape)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        vec_a = self.X[:,:,:,:,i]\n",
    "        vec_b = self.Y[:,:,:,:,i]\n",
    "        vec_a = np.transpose(vec_a, (3, 0, 1, 2))\n",
    "        vec_b = np.transpose(vec_b, (3, 0, 1, 2))\n",
    "        a = torch.Tensor(vec_a)\n",
    "        b = torch.Tensor(vec_b.squeeze())\n",
    "    \n",
    "        return a,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shset = SHDataSet(X,Y)\n",
    "print(len(shset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #self.fc1 = nn.Linear(3 * 3 * 3 * 15, 600)\n",
    "        self.cn1 = nn.Conv3d(31,128,kernel_size=(3,3,3),stride=1,padding=(1,1,1))\n",
    "        self.cn2 = nn.Conv3d(128,128,kernel_size=(3,3,3),stride=1,padding=(1,1,1))\n",
    "        ## self.mp = nn.MaxPool(2)\n",
    "        #print(self.cn1.shape)\n",
    "        #self.fc2 = nn.Linear(600,300)\n",
    "        self.bn = nn.BatchNorm3d(128)\n",
    "        self.fc2 = nn.Linear(128*5*5*5, 300)\n",
    "        self.fc3 = nn.Linear(300,60)\n",
    "        self.fc4 = nn.Linear(60,200)\n",
    "        self.fc5 = nn.Linear(200,30)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.cn1(x))\n",
    "        x = F.relu(self.cn2(x))\n",
    "        ## x = self.mp(x)\n",
    "        print(x.shape)\n",
    "        dimensions = x.shape\n",
    "        x = self.bn(x)\n",
    "        x = x.view(dimensions[0], -1)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "        \n",
    "net = Net()\n",
    "print(net)  \n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "\n",
    "def train(model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = output.to(device)\n",
    "\n",
    "        loss = criterion(output,target)\n",
    "        total_loss += loss.item()\n",
    "      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx*len(data),len(shset.X),100.*batch_idx/len(shset.X),loss.data[0]))\n",
    "\n",
    "    avg_loss = total_loss / batch_idx\n",
    "    print('\\tTraining set: Average loss: {:.4f}'.format(avg_loss))\n",
    "   \n",
    "    return avg_loss\n",
    "\n",
    "def test(model, device, loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = Variable(data).float(), Variable(target).float()\n",
    "\n",
    "        output = model(data)\n",
    "        output = output.to(device)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / batch_idx\n",
    "    print('\\tTesting set: Average loss: {:.4f}'.format(avg_loss))\n",
    "\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(shset)\n",
    "print(dataset_size)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(shset, batch_size=bs_size, sampler=train_sampler)\n",
    "validation_loader = DataLoader(shset, batch_size=bs_size, sampler=valid_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_loss_file = 'BLAHFCN_L5PATCH_A2D_train_loss_split.txt'\n",
    "f = open(train_loss_file, 'w')\n",
    "f.close()\n",
    "validate_loss_file = 'BLAHFCN_L5PATCH_A2D_validate_loss_split.txt'\n",
    "f = open(validate_loss_file, 'w')\n",
    "f.close()\n",
    "\n",
    "model_file = 'BLAHFCN_L5PATCH_A2D_saved_model_split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 51):\n",
    "        print('\\nEpoch %d: ' % epoch)\n",
    "        loss = train(net, device, train_loader, optimizer)\n",
    "\n",
    "        with open(train_loss_file, \"a\") as file:\n",
    "            file.write(str(loss))\n",
    "            file.write('\\n')\n",
    "\n",
    "        loss = test(net, device, validation_loader)\n",
    "\n",
    "        with open(validate_loss_file, \"a\") as file:\n",
    "            file.write(str(loss))\n",
    "            file.write('\\n')\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            with open(model_file, 'wb') as f:\n",
    "                torch.save(net.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
